# -*- coding: utf-8 -*-
"""BERTVALTESTTRAIN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L9IfG68hcp5lk21Z9N4FbhUQPPvlLLzt
"""

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
#APIkey = userdata.get('APIkey')

!pip install datasets transformers huggingface_hub
!apt-get install git-lfs
!pip install evaluate

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import torch
import re
import nltk
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Load dataset
train = pd.read_csv("/content/drive/MyDrive/train.csv")
test = pd.read_csv("/content/drive/MyDrive/test.csv")

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
stop_words.discard('not')  # Keep negation words

# Precompile regex patterns
MENTIONS_PATTERN = re.compile(r'@\w+')
URL_PATTERN = re.compile(r'https?://\S+|www\.\S+')
SPECIAL_CHARS_PATTERN = re.compile(r'[^a-zA-Z\s]')  # Keep only letters and spaces
EXTRA_SPACES_PATTERN = re.compile(r'\s+')

def preprocess_tweet(tweet: str) -> str:
    tweet = MENTIONS_PATTERN.sub('', tweet)  # Remove @mentions
    tweet = URL_PATTERN.sub('', tweet)       # Remove URLs
    tweet = SPECIAL_CHARS_PATTERN.sub(' ', tweet)  # Remove special chars
    tweet = tweet.lower().split()
    tweet = [lemmatizer.lemmatize(word) for word in tweet if word not in stop_words]
    return EXTRA_SPACES_PATTERN.sub(' ', ' '.join(tweet)).strip()

# Apply preprocessing
train['cleaned_tweet'] = train['tweet'].apply(preprocess_tweet)
test['cleaned_tweet'] = test['tweet'].apply(preprocess_tweet)

# Label Mapping
label_mapping = {"NOT": 0, "TIN": 1, "UNT": 2}
train['label_mapped'] = train['label'].map(label_mapping)

# Train-validation split
X_train, X_val, y_train, y_val = train_test_split(train['cleaned_tweet'], train['label_mapped'], test_size=0.3, random_state=100)

# Convert to Hugging Face dataset
train_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_train, 'label': y_train}))
val_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_val, 'label': y_val}))
test_dataset = Dataset.from_pandas(pd.DataFrame({'text': test['cleaned_tweet']}))

# Tokenization
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
def preprocess_function(examples):
    return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)
tokenized_test = test_dataset.map(preprocess_function, batched=True)

# Load Model
num_labels = len(label_mapping)
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=num_labels)

# Training arguments
training_args = TrainingArguments(
    output_dir="bert_model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    save_strategy="epoch"
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average='weighted')
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train Model
trainer.train()

# Evaluate Model
eval_results = trainer.evaluate()
print("Evaluation Results:", eval_results)

# Make Predictions on Test Data
predictions = trainer.predict(tokenized_test).predictions
predicted_labels = np.argmax(predictions, axis=-1)
test['prediction'] = predicted_labels

# Save Predictions
test[['id', 'prediction']].to_csv('/content/drive/MyDrive/test-predictions.csv', index=False)
print("Predictions saved to test-predictions.csv")