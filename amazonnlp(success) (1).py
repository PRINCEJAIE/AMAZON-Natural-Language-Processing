# -*- coding: utf-8 -*-
"""amazonnlp(success).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13iLPo7m48B7t-x54y1iRKAcZ660vyrZ6

## **Source Code**

https://colab.research.google.com/drive/1L9IfG68hcp5lk21Z9N4FbhUQPPvlLLzt?usp=sharing

## **YouTube Link**
https://youtu.be/nVKiPsKV6M8
"""

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets transformers huggingface_hub
!apt-get install git-lfs
!pip install evaluate pyspellchecker emoji contractions tqdm pandarallel

import pandas as pd
import numpy as np
import re
import os
import torch
import warnings
warnings.filterwarnings('ignore')
import nltk
from bs4 import BeautifulSoup
import emoji
import contractions
from spellchecker import SpellChecker
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from tqdm import tqdm
from pandarallel import pandarallel
pandarallel.initialize(progress_bar=True)
tqdm.pandas()
nltk.download('stopwords')
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
stop_words.discard('not')

df = pd.read_csv('/content/drive/MyDrive/Amazon Review DL/AmazonReviews.csv')
df

df1 = df[['Score', 'Summary', 'Text']]
df1.drop_duplicates(subset=['Text'], inplace=True)
df1.dropna(axis=0, inplace=True)
df1 = df1[:10000]  # Use a sample of 10,000 rows for faster processing

contraction_mapping = {
    "ain't": "is not", "aren't": "are not", "can't": "cannot", "'cause": "because",
    "could've": "could have", "couldn't": "could not", "didn't": "did not",
    "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not",
    "haven't": "have not", "he'd": "he would", "he'll": "he will", "he's": "he is",
    "I'd": "I would", "I'll": "I will", "I'm": "I am", "I've": "I have",
    "isn't": "is not", "it's": "it is", "let's": "let us", "might've": "might have",
    "must've": "must have", "shan't": "shall not", "she'd": "she would",
    "she's": "she is", "should've": "should have", "shouldn't": "should not",
    "that's": "that is", "there's": "there is", "they'd": "they would",
    "they'll": "they will", "they're": "they are", "we'd": "we would",
    "we're": "we are", "we've": "we have", "weren't": "were not", "what's": "what is",
    "where's": "where is", "who's": "who is", "won't": "will not", "would've": "would have",
    "wouldn't": "would not", "you're": "you are", "you've": "you have"
}

contraction_pattern = re.compile(r'\b(' + '|'.join(contraction_mapping.keys()) + r')\b') # maaping the contractions and giving it its own custom function
def expand_contractions_vectorized(text_series):
    return text_series.str.replace(contraction_pattern, lambda x: contraction_mapping[x.group()], regex=True)

df1['Text'] = expand_contractions_vectorized(df1['Text'])
df1['Summary'] = expand_contractions_vectorized(df1['Summary'])

spell = SpellChecker()

def text_cleaned(text):
  text = text.lower().strip() # lowercase and strips whitespace
  text = BeautifulSoup(text, 'html.parser').get_text() # removes html tags
  text = re.sub(r'[^a-z\s]', '', text) # removes punctuation and special characters
  text = re.sub(r'\b\w{1,2}\b', '', text) # removes short words that are 1-2 in length
  if len(text) <=100: # correct spelling only if words is short
    corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in text.split()]
    text = ' '.join(corrected_words)
  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]) # lemmatization
  text = re.sub(r'\s+', ' ', text) # removes mutiple spaces and trims
  return text

df1['Cleaned_Text'] = df1['Text'].parallel_apply(text_cleaned)
df1['Cleaned_Summary'] = df1['Summary'].parallel_apply(text_cleaned)

def map_score(score):
    if score in [1, 2, 3]:
        return 0
    elif score in [4, 5]:
        return 1

df1['label']= df1['Score'].parallel_apply(map_score)



# df_cd = df1[['Cleaned_Text', 'Cleaned_Summary', 'label']]
# df_cd.rename(columns={'Cleaned_Text': 'Text_CL', 'Cleaned_Summary': 'Summary_CL', 'label': 'Label_CL'}, inplace=True)

# df_cd.to_csv('/content/drive/MyDrive/Amazon Review DL/cleaned_data.csv', index=False) this has already been ran to se

# df123 =pd.read_csv('/content/drive/MyDrive/Amazon Review DL/cleaned_data.csv')
# df123

"""##**HugginFace intergration**


you can see the creation of the HuggingFace Dataset through this link: https://colab.research.google.com/drive/1MZncVnOHCmrmEbzKOl_LjgvsLcoMGX15?usp=sharing
"""

from datasets import load_dataset
dataset = load_dataset("PRINCEEMMANUEL/PREPROCESS_AMAZON") # this is the same

dataset

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def preprocess_function(examples):
    tokenized_inputs = tokenizer(examples["Text_CL"], truncation=True, padding ="max_length", max_length=512)
    tokenized_inputs["labels"] = examples["Label_CL"]
    return tokenized_inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True)

print(tokenized_dataset["train"][0])

tokenized_train = tokenized_dataset["train"]
tokenized_test = tokenized_dataset["test"]
tokenized_valid = tokenized_dataset["valid"]

from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

import numpy as np
import evaluate
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average='weighted')
    }

from huggingface_hub import notebook_login

notebook_login()

from transformers import TrainingArguments, Trainer

repo_name = "finetuning-amazon-reviews"

training_args = TrainingArguments(
    output_dir=repo_name,
    learning_rate=2e-05,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    o
    weight_decay=0.01,
    save_strategy="epoch",
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate Model
eval_results = trainer.evaluate()
print("Evaluation Results:", eval_results)

